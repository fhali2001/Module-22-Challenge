# Module-22-Challenge

# Home Sales Data Analysis with SparkSQL

This project aims to analyze home sales data using SparkSQL to derive key metrics. It involves tasks such as creating temporary views, partitioning data, caching, and uncaching temporary tables.

## Getting Started

1. **Clone Repository**: Clone the Home_Sales repository to your local machine.
2. **File Download**: Download the [Module 22 Challenge files](https://static.bc-edx.com/data/dl-1-2/m22/lms/starter/Starter_Code.zip) to help you get started.

## Requirements

- Python
- PySpark
- Jupyter Notebook

## Instructions

1. **Rename File**: Rename the `Home_Sales_starter_code.ipynb` file as `Home_Sales.ipynb`.
2. **Import Libraries**: Import the necessary PySpark SQL functions for this assignment.
3. **Read Data**: Read the `home_sales_revised.csv` data into a Spark DataFrame.
4. **Create Temporary Table**: Create a temporary table called `home_sales`.
5. **Answer Questions**: Answer the provided questions using SparkSQL queries:
    - What is the average price for a four-bedroom house sold for each year? Round off your answer to two decimal places.
    - What is the average price of a home for each year the home was built, that has three bedrooms and three bathrooms? Round off your answer to two decimal places.
    - What is the average price of a home for each year the home was built, that has three bedrooms, three bathrooms, two floors, and is greater than or equal to 2,000 square feet? Round off your answer to two decimal places.
    - What is the average price of a home per "view" rating having an average home price greater than or equal to $350,000? Determine the run time for this query, and round off your answer to two decimal places.
6. **Cache Temporary Table**: Cache your temporary table `home_sales`.
7. **Check Cache**: Check if your temporary table is cached.
8. **Run Cached Query**: Using the cached data, run the last query that calculates the average price of a home per "view" rating having an average home price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.
9. **Partition Data**: Partition by the "date_built" field on the formatted parquet home sales data.
10. **Create Parquet Table**: Create a temporary table for the parquet data.
11. **Run Parquet Query**: Run the last query that calculates the average price of a home per "view" rating having an average home price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.
12. **Uncache Table**: Uncache the `home_sales` temporary table.
13. **Verify Uncache**: Verify that the `home_sales` temporary table is uncached using PySpark.
14. **Submission**: Download your `Home_Sales.ipynb` file and upload it into your "Home_Sales" GitHub repository.

## Support and Resources

- Your instructional team will provide support during classes and office hours.
- Access to learning assistants and tutors is available for additional help.

## Submission

To submit your Challenge assignment, provide the URL of your GitHub repository for grading.

## Grade Table

This project will be evaluated against the requirements and assigned a grade according to the following table:

| Grade  | Points |
| ------ | ------ |
| A (+/-)| 90+    |
| B (+/-)| 80–89  |
| C (+/-)| 70–79  |
| D (+/-)| 60–69  |
| F (+/-)| < 60   |

## Important Note

Ensure to include a note in the README section of your repo specifying code sources and their locations within your repo if you've collaborated with a peer, used external code sources, or received code assistance from instructional staff.

### References

Data for this dataset was generated by edX Boot Camps LLC, and is intended for educational purposes only.
